{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/mpuig/d4a69eb1f08f24a7f5f36748bc300edb/chatbot-summit-berlin-2024-workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "The processing and analysis of customer service interactions are essential for understanding customer needs and improving service delivery. This project aims to use LLMs to analyze customer call transcripts effectively.\n",
        "\n",
        "## Use Case: Insights from Call Transcripts\n",
        "\n",
        "The goal is to analyze customer interactions to understand the nuances of customer interactions. By doing so, we aim to:\n",
        "\n",
        "- **Objective**: Analyze customer conversations to uncover underlying needs, concerns, and perceptions.\n",
        "- **Benefit**: Provide insights for data-driven decisions and strategy refinement.\n",
        "- **Application**: Employ LLMs to efficiently process, analyze, and interpret the vast amounts of data contained in call transcripts. This will enable us to identify key trends, gauge sentiment, and detect compliance or training gaps.\n",
        "\n",
        "## User Story: Predictive Analytics for Risk Management\n",
        "\n",
        "Consider the perspective of a Data Scientist working in the Risk Management Department, tasked with a critical challenge:\n",
        "\n",
        "```\n",
        "As a Data Scientist in the Risk Management Department,\n",
        "I need to apply predictive analytics to customer call transcripts,\n",
        "To predict client compliance with credit obligations.\n",
        "```\n",
        "\n",
        "Success in this area means better-targeted interventions and risk management strategies, leading to more personalized communication and efficient resource allocation.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "A brief overview of relevant terms:\n",
        "\n",
        "- **EMI (Equated Monthly Installment)**: Regular payments made to repay a loan over a designated period.\n",
        "\n"
      ],
      "metadata": {
        "id": "3u2vZkez721R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment setup\n",
        "\n",
        "- Install necessary python packages\n",
        "- Get some useful secrets from Google Colab userdata\n",
        "- Set the environment variables to use OpenAI LLMs"
      ],
      "metadata": {
        "id": "GkBiYHWZ2Gad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install langchain==0.1.11 langchain-core==0.1.29 langchain-community==0.0.25 langchain-openai==0.0.8 --quiet\n",
        "!pip3 install simplejson pydantic --quiet"
      ],
      "metadata": {
        "id": "DxK_qFvlA1xx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a3bc2d-36e9-4ea5-b785-9504c9a13506"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.6/252.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.9/70.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import simplejson as json\n",
        "\n",
        "from google.colab import userdata\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Language models to use\n",
        "GPT4_0125 = \"gpt-4-0125-preview\"\n",
        "GPT35TURBO0125 = \"gpt-3.5-turbo-0125\"\n",
        "\n",
        "# OpenAI SECRETS\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "OPENAI_API_ORG = userdata.get('OPENAI_API_ORG')\n",
        "\n",
        "print(f\"OpenAI API Key: sk-...{OPENAI_API_KEY[-5:]}\")\n",
        "print(f\"OpenAI API Org: org-...{OPENAI_API_ORG[-5:]}\")"
      ],
      "metadata": {
        "id": "vJ1Kd4WnBmDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab2194a7-975a-4f41-87ff-8028594f9e85"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API Key: sk-...3kTNI\n",
            "OpenAI API Org: org-...TrCdV\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample Data\n",
        "\n",
        "Below is a sample call transcript from a financial services call center interaction between an agent and a client. This transcript will serve as our primary example throughout the workshop."
      ],
      "metadata": {
        "id": "VW28PgdSFqjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CALL_TRANSCRIPT = \"\"\"\\\n",
        "Agent: I'm calling from Global United Bank regarding your two-wheeler loan.\n",
        "Customer: Who is speaking?\n",
        "Agent: My name is Emily Roberts, calling from Global United Bank.\n",
        "Customer: Yes.\n",
        "Agent: To confirm, I'm speaking with John Smith?\n",
        "Customer: Yes, yes.\n",
        "Agent: This call is being recorded for quality and training purposes. It appears you have an outstanding two-wheeler loan payment.\n",
        "Customer: Yes, yes.\n",
        "Agent: The last four digits of your loan account are 6059. Your EMI was due on the 3rd, and you have yet to make a payment of 120€.\n",
        "Customer: I understand. I've been out of town, but my son will return in two days to make the payment.\n",
        "Agent: It's important to make the payment as soon as possible. Have you considered using Phone Pay or Google Pay for an immediate payment?\n",
        "Customer: I will discuss this with my son as soon as I return home.\n",
        "Agent: Please, let's try to resolve this now. Can we initiate the payment during this call?\n",
        "Customer: I will speak with my son and ensure it's done.\n",
        "Agent: I urge you to make the payment now for immediate resolution. Shall we proceed with setting up the payment?\n",
        "Customer: I will handle it, thank you.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QgsdS_f4Fpe5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example the customer does not explicitly demonstrate financial inability, express dissatisfaction with the bank or the product, or refuse to make a payment.\n",
        "\n",
        "Instead, the customer acknowledges the payment due and mentions a plan to make the payment (\"*my son will return in two days to make the payment*\"). This response suggests an intention to pay but indicates a delay due to personal circumstances, possibly indicating forgetfulness or a temporary inability to make the payment immediately due to being out of town."
      ],
      "metadata": {
        "id": "0Ec7TA_4iGqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1st Approach - Brute Force with GPT-4\n",
        "\n",
        "The initial strategy is to use brute force analysis with GPT-4, since it is (apparently) super smart.\n"
      ],
      "metadata": {
        "id": "uFt8TzNtFvdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompts"
      ],
      "metadata": {
        "id": "jtYvQYeNGAIH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yvxbfrjD7htt"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT_TEMPLATE = \"\"\"\\\n",
        "You are a top expert in analyzing and understanding customers' call responses\n",
        "to bank agents with regards to the customer not yet paying their loan dues.\n",
        "You are also an expert in accurately identifying a customer's promise to pay\n",
        "and the corresponding details of the promise.\n",
        "\"\"\"\n",
        "\n",
        "USER_PROMPT_TEMPLATE = \"\"\"\\\n",
        "Examine the following call transcript between a bank agent and a customer (delimited by <ctx></ctx>).\n",
        "\n",
        "<ctx>\n",
        "{transcript}\n",
        "</ctx>\n",
        "\n",
        "The goal is to categorize the call strictly into one of the following list of categories:\n",
        "- Inability to pay: the customer demonstrated financial inability to pay the outstanding loan dues\n",
        "- Discipline to pay: the customer demonstrated discipline to pay outstanding loan dues.\n",
        "- Unwilling to pay: the customer demonstrated unwillingness to pay outstanding loan dues.\n",
        "\n",
        "Just return the category, without any additional details. For example: Inability to pay\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains\n",
        "\n",
        "For the following examples we'll use LCEL (LangChain Expression Language), that makes it easy to build complex chains from basic components.\n",
        "\n",
        "More info here: https://python.langchain.com/docs/expression_language"
      ],
      "metadata": {
        "id": "rCuZzIEsGxr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", SYSTEM_PROMPT_TEMPLATE),\n",
        "    (\"user\", USER_PROMPT_TEMPLATE)\n",
        "])\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=GPT35TURBO0125,\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    openai_organization=OPENAI_API_ORG,\n",
        "    temperature=0,\n",
        "    model_kwargs={\"seed\": 42},\n",
        ")\n",
        "\n",
        "# The output of a ChatModel (and therefore, of this chain) is a message.\n",
        "# However, it's often much more convenient to work with strings.\n",
        "# Let's add a simple output parser to convert the chat message to a string.\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# We can now combine these into a simple LLM chain:\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "# In this chain the user input is passed to the prompt template, then the prompt template output is passed to\n",
        "# the model, then the model output is passed to the output parser.\n",
        "result = chain.invoke({\"transcript\": CALL_TRANSCRIPT})\n",
        "\n",
        "print(f\"Response -> {result}\")"
      ],
      "metadata": {
        "id": "905U5UFiF2XZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "154ba218-c544-4ef9-9134-5d1cde31a786"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response -> Discipline to pay\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function calls\n",
        "\n",
        "Using OpenAI function calls for parsing results offers a blend of flexibility that can significantly enhance the ability to process and analyze text data. More info: https://platform.openai.com/docs/guides/function-calling"
      ],
      "metadata": {
        "id": "kX1jhU60HSvD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our endeavor to understand customer interactions and commitments regarding loan payments, we introduce a structured function definition, `FUNCTION_REASON_NO_PAY`. This function is designed to assess whether a customer has made any form of commitment or promise to pay their outstanding loan dues.\n"
      ],
      "metadata": {
        "id": "bUgvTh-VroDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FUNCTION_REASON_NO_PAY = {\n",
        "    \"name\": \"reason_no_pay\",\n",
        "    \"description\": \"Determine whether the customer made some form of a promise to pay the loan dues\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"answer\": {\n",
        "                \"type\": \"string\",\n",
        "                \"enum\": [\"yes\", \"no\"],\n",
        "                \"description\": \"Answer Yes or No as to whether a promise to pay the outstanding loan dues was made\",\n",
        "            },\n",
        "            \"details\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"Medium-form detailed summary of the customer's promise to pay outstanding loan dues this month\",\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"answer\", \"details\"]\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "sA6FSHK0FDDp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To apply the function `reason_no_pay` as part of a sequence of operations or \"Runnables\" we can use `Runnable.bind()`. It invokes a Runnable within a Runnable sequence with constant arguments that are not part of the output of the preceding Runnable in the sequence, and which are not part of the user input."
      ],
      "metadata": {
        "id": "iC_Bc84LsEuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "\n",
        "openai_functions = [FUNCTION_REASON_NO_PAY]\n",
        "\n",
        "bind = llm.bind(\n",
        "    function_call={\"name\": \"reason_no_pay\"},\n",
        "    functions=openai_functions,\n",
        ")\n",
        "\n",
        "chain = prompt | bind | JsonOutputFunctionsParser()\n",
        "\n",
        "result = chain.invoke({\"transcript\": CALL_TRANSCRIPT})\n",
        "\n",
        "print(json.dumps(result, indent=4, sort_keys=True))"
      ],
      "metadata": {
        "id": "5rtthAgNIMom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d996bff-0c2e-445e-c8bf-729af4e56c6b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"answer\": \"no\",\n",
            "    \"details\": \"The customer mentioned that their son will return in two days to make the payment and they will handle it.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Calls based on Pydantic Models\n",
        "\n",
        "Since we're in an OpenAI Context, we can use Pydantic models to define a function call and also to validate the obtained results.\n",
        "\n",
        "The `pydantic` model serves as a structured way to validate and document the expected input and output for such operations, ensuring that data passed to and from the API meets the application's requirements."
      ],
      "metadata": {
        "id": "-_0DVqGnJboq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing import Literal\n",
        "\n",
        "\n",
        "class QuestionReasonNoPay(BaseModel):\n",
        "    \"\"\"Identify and summarize reason for the customer not yet paying the loan dues for this month\"\"\"\n",
        "\n",
        "    answer: Literal[\"yes\", \"no\"] = Field(\n",
        "        description=\"Whether a reason was provided by the customer for them not yet paying the loan dues\"\n",
        "    )\n",
        "    details: str = Field(\n",
        "        description=\"Medium-form detailed summary of the customer's reason for not yet paying loan dues\"\n",
        "    )"
      ],
      "metadata": {
        "id": "IgORkXUMJktw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "openai_functions = [convert_to_openai_function(QuestionReasonNoPay)]\n",
        "\n",
        "bind = llm.bind(\n",
        "    function_call={\"name\": \"QuestionReasonNoPay\"},\n",
        "    functions=openai_functions\n",
        ")\n",
        "\n",
        "chain = prompt | bind | JsonOutputFunctionsParser()\n",
        "\n",
        "result = chain.invoke({\"transcript\": CALL_TRANSCRIPT})\n",
        "\n",
        "QuestionReasonNoPay(**result)\n",
        "\n",
        "# Check that we obtain the same function call object than in the previous step\n",
        "# print(json.dumps(openai_functions, indent=4, sort_keys=True))"
      ],
      "metadata": {
        "id": "ORAUIFees0W8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c41021-8853-4064-b8ba-5f5c6ac313d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuestionReasonNoPay(answer='no', details='The customer did not provide a reason for not yet paying the loan dues.')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extending the scope\n",
        "\n",
        "We want to add more questions to the initial one:\n",
        " - Whether customer demonstrated discipline to pay outstanding loan dues\n",
        " - Whether customer demonstrated financial inability to pay outstanding loan due\n",
        " - Whether customer demonstrated unwillingness to pay outstanding loan dues\n",
        "\n",
        " The first attempt will consist in implementing it as a function call, but we can guess that this won't be the final option."
      ],
      "metadata": {
        "id": "8v2BhP0j3JaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QuestionReasonNoPayExtended(BaseModel):\n",
        "    \"\"\"Identify and summarize reason for the customer not yet paying the loan dues for this month\"\"\"\n",
        "\n",
        "    # Original question, renamed\n",
        "    reason_no_pay: Literal[\"yes\", \"no\"] = Field(\n",
        "        description=\"Whether a reason was provided by the customer for them not yet paying the loan dues\"\n",
        "    )\n",
        "    reason_no_pay_details: str = Field(\n",
        "        description=\"Mid-form detailed summary of the customer's reason for not yet paying loan dues\"\n",
        "    )\n",
        "\n",
        "    # New questions here:\n",
        "\n",
        "    # Inability to pay\n",
        "    inability_pay: Literal[\"yes\", \"no\"] = Field(\n",
        "        description=\"Whether customer demonstrated inability to pay outstanding loan dues\"\n",
        "    )\n",
        "    inability_pay_details: str = Field(\n",
        "        description=\"Explanation for the inability_pay answer\"\n",
        "    )\n",
        "\n",
        "    # Discipline to pay\n",
        "    discipline_pay: Literal[\"yes\", \"no\"] = Field(\n",
        "        description=\"Whether customer demonstrated discipline to pay outstanding loan dues\"\n",
        "    )\n",
        "    discipline_pay_details: str = Field(\n",
        "        description=\"Explanation for the discipline_pay answer\"\n",
        "    )\n",
        "\n",
        "    # Unwilling to Pay\n",
        "    unwilling_pay: Literal[\"yes\", \"no\"] = Field(\n",
        "        description=\"Whether customer demonstrated unwillingness to pay outstanding loan dues\"\n",
        "    )\n",
        "    unwilling_pay_details: str = Field(\n",
        "        description=\"Explanation for the unwilling_pay answer\"\n",
        "    )"
      ],
      "metadata": {
        "id": "X2ZjDfyIKNXT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_functions = [convert_to_openai_function(QuestionReasonNoPayExtended)]\n",
        "\n",
        "bind = llm.bind(\n",
        "    function_call={\"name\": \"QuestionReasonNoPayExtended\"},\n",
        "    functions=openai_functions\n",
        ")\n",
        "\n",
        "chain = prompt | bind | JsonOutputFunctionsParser()\n",
        "\n",
        "result = chain.invoke({\"transcript\": CALL_TRANSCRIPT})\n",
        "\n",
        "QuestionReasonNoPayExtended(**result)"
      ],
      "metadata": {
        "id": "Pg9We_ETN-qe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85e78aa1-5da8-4d22-c20d-0afaa0ec440a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuestionReasonNoPayExtended(reason_no_pay='yes', reason_no_pay_details='Customer mentioned being out of town but son will return in two days to make the payment.', inability_pay='no', inability_pay_details='Customer indicated son will make the payment in two days.', discipline_pay='no', discipline_pay_details='Customer did not show immediate willingness to make the payment during the call.', unwilling_pay='no', unwilling_pay_details='Customer expressed intention to handle the payment later.')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outcome\n",
        "\n",
        "- High computational demands leading to slow response times.\n",
        "- Difficulty in extracting specific insights due to the broad analysis scope."
      ],
      "metadata": {
        "id": "rPgYA03e6ArR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2nd Approach - Divide and Conquer\n",
        "\n",
        "However, a one-size-fits-all approach to questioning can lead to inefficiencies and may not capture the nuanced perspectives of different customers. To address this, we propose introducing a more dynamic questioning strategy, focusing on directing follow-up questions that are relevant based on the customer's initial responses. By tailoring our inquiries to align with the customer's specific concerns—as indicated by their previous \"YES\" answers—we can gather more targeted and meaningful insights."
      ],
      "metadata": {
        "id": "xgXno2cDNAcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Follow-up Questions\n",
        "\n",
        "We define a new user prompt, more generic than the previous one, and that accepts a `question` as a parameter."
      ],
      "metadata": {
        "id": "s4khHjIlwSJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USER_PROMPT_TEMPLATE_WITH_QUESTION = \"\"\"\\\n",
        "Examine the following call transcript between a bank agent and a customer (delimited by <ctx></ctx>).\n",
        "\n",
        "<ctx>\n",
        "{transcript}\n",
        "</ctx>\n",
        "\n",
        "Think step-by-step in accurately answering the following question, and answer yes or no:\n",
        "\n",
        "```\n",
        "{question}\n",
        "```\n",
        "\n",
        "Do not ever give or use any information not mentioned in the transcript in your answers.\n",
        "\"\"\"\n",
        "\n",
        "prompt_with_question = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", SYSTEM_PROMPT_TEMPLATE),\n",
        "    (\"user\", USER_PROMPT_TEMPLATE_WITH_QUESTION)\n",
        "])"
      ],
      "metadata": {
        "id": "dFTkZZB7U924"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Question` class provides a blueprint for constructing questions and allows for a uniform approach to generating follow-up questions."
      ],
      "metadata": {
        "id": "aF3vq2h3Dnh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Question(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    answer_description: str\n",
        "    details_description: str"
      ],
      "metadata": {
        "id": "SNCbnU5TwYCZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `BinaryAnswer` model, is a Pydantic model designed to capture and structure the outcomes of our function calls:\n"
      ],
      "metadata": {
        "id": "9DFjxMddFqVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryAnswer(BaseModel):\n",
        "    name: str = Field(description=\"The identifier of the question being addressed\")\n",
        "    answer: Literal[\"yes\", \"no\"] = Field(description=\"A binary response to the question, restricted to yes or no.\")\n",
        "    details: str = Field(description=\"Further elaboration on the answer provided.\")"
      ],
      "metadata": {
        "id": "Poqgz_AEpd-g"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An array, `QUESTIONS`, contains instances of the `Question` class, each representing a specific follow-up question that may be relevant based on the initial response."
      ],
      "metadata": {
        "id": "AVgpmUFKwafS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTIONS = [\n",
        "    Question(\n",
        "        name=\"inability_to_pay\",\n",
        "        description=\"Determine whether customer demonstrated financial inability to pay the outstanding loan dues\",\n",
        "        answer_description=\"Whether customer demonstrated discipline to pay outstanding loan dues\",\n",
        "        details_description=\"Explanation for the answer\"\n",
        "    ),\n",
        "    Question(\n",
        "        name=\"discipline_to_pay\",\n",
        "        description=\"Determine whether customer demonstrated discipline to pay the outstanding loan dues\",\n",
        "        answer_description=\"Whether customer demonstrated discipline to pay outstanding loan dues\",\n",
        "        details_description=\"Explanation for the answer\",\n",
        "    ),\n",
        "    Question(\n",
        "        name=\"unwilling_to_pay\",\n",
        "        description=\"Determine whether customer demonstrated unwillingness to pay the outstanding loan dues\",\n",
        "        answer_description=\"Whether customer demonstrated unwillingness to pay outstanding loan dues\",\n",
        "        details_description=\"Explanation for the answer\",\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "LPflqqrOR_iK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `generate_function` takes a `Question` object as its input and produces a structured dictionary that corresponds to the OpenAI function call format:"
      ],
      "metadata": {
        "id": "1Qbd4Ootwn6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_function(question: Question) -> dict:\n",
        "    return {\n",
        "        \"name\": question.name,\n",
        "        \"description\": question.description,\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"answer\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"enum\": [\"yes\", \"no\"],\n",
        "                    \"description\": question.answer_description,\n",
        "                },\n",
        "                \"details\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": question.details_description,\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"answer\", \"details\"]\n",
        "        }\n",
        "    }"
      ],
      "metadata": {
        "id": "HGUK-fKRpTIw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the aim to obtain structured insights from the customer call transcript when the initial analysis yields a \"yes\" response, we proceed to integrate the previously defined `questions` and the `generate_function` function into a practical workflow. This workflow will dynamically generate and execute OpenAI function calls based on specific follow-up questions and processes the results.\n",
        "\n"
      ],
      "metadata": {
        "id": "srjtA3py9lgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing Workflow\n",
        "\n",
        "The workflow proceeds as follows, assuming that an initial question (`question1`) has been asked and answered with \"yes\":\n",
        "\n",
        "1. **Iterate Through Follow-up Questions**: For each question in our predefined list (`QUESTIONS`), we dynamically construct a function call using the `generate_function` utility.\n",
        "\n",
        "2. **Binding and Execution**:\n",
        "   - For each question, we bind its corresponding OpenAI function call (including name and structured parameters) using `llm.bind`.\n",
        "   - We then set up a chain of operations starting with a prompt that includes the question, followed by the bound function call, and finally, a parser (`JsonOutputFunctionsParser`) to interpret the JSON output.\n",
        "\n",
        "3. **Invoke and Collect Results**:\n",
        "   - The chain is invoked with the current transcript and the specific question being addressed.\n",
        "   - The results are then structured according to the `BinaryAnswer` model, capturing the name of the question and the details of the response.\n"
      ],
      "metadata": {
        "id": "IvKtxj0GxsV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We go back to the QuestionReasonNoPay, with just one yes/no question\n",
        "openai_functions = [convert_to_openai_function(QuestionReasonNoPay)]\n",
        "\n",
        "bind = llm.bind(\n",
        "    function_call={\"name\": \"QuestionReasonNoPay\"},\n",
        "    functions=openai_functions\n",
        ")\n",
        "\n",
        "chain = prompt | bind | JsonOutputFunctionsParser()\n",
        "\n",
        "result = chain.invoke({\"transcript\": CALL_TRANSCRIPT})\n",
        "\n",
        "question1 = QuestionReasonNoPay(**result)\n",
        "\n",
        "results = []\n",
        "if question1.answer == \"yes\":\n",
        "    for question in QUESTIONS:\n",
        "        bind = llm.bind(\n",
        "            function_call={\"name\": question.name},\n",
        "            functions=[generate_function(question)],\n",
        "        )\n",
        "        chain = prompt_with_question | bind | JsonOutputFunctionsParser()\n",
        "        result = chain.invoke({\"transcript\": CALL_TRANSCRIPT, \"question\": question})\n",
        "        results.append(BinaryAnswer(name=question.name, **result))\n",
        "\n",
        "\n",
        "# Print the initial question's response with context\n",
        "print(f\"Initial Question Response:\\n{question1}\\n\")\n",
        "\n",
        "# Iterate through each result in the results list, printing with added clarity\n",
        "for result in results:\n",
        "    print(f\"Question: {result.name}\")\n",
        "    print(f\"Answer: {result.answer}\")\n",
        "    # Optionally, check if 'details' has meaningful content before printing\n",
        "    if result.details.strip():\n",
        "        print(f\"Details: {result.details}\")\n",
        "    print(\"-\" * 40)  # Add a separator for readability"
      ],
      "metadata": {
        "id": "wNvw1dXmOIYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "813c9169-b6f7-42ff-aeaa-104aeef65124"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Question Response:\n",
            "answer='no' details='The customer did not provide a reason for not yet paying the loan dues.'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outcome\n",
        "\n",
        "- Marginal improvements in manageability but increased operational complexity and costs.\n",
        "- Still slow and expensive.\n",
        "- Segmented analysis complicated the synthesis of comprehensive insights."
      ],
      "metadata": {
        "id": "pRVrvVXO-li-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3rd Approach - Logic Flow\n",
        "\n",
        "From the previous steps we can simplify the process, making it more manegeable and interpretable, structuring the queries into a logical flow of yes/no questions, facilitating easier evaluation.\n",
        "\n",
        "With a logic flow, we can decide to use more affordable LLM option like GPT3.5 to some questions. It'll reduce operational costs without significantly compromising analytical capabilities, making it a better choice for large(ish)-scale analysis.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "igkVLW7RlRKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Pipeline Configuration\n",
        "\n",
        "As an example we'll create the following (simple) pipeline:\n",
        "\n",
        "```\n",
        "#\n",
        "#        ┌--Y--> [inability_to_pay, discipline_to_pay, unwilling_to_pay]\n",
        "#        |\n",
        "# --> reason_no_pay\n",
        "#        |\n",
        "#        └--N--> None\n",
        "#\n",
        "```\n",
        "\n",
        "A more complex one can contain:\n",
        "\n",
        "- Add extraction modules to add data processing capabilities per step\n",
        "- Specify different prompt templates per step\n",
        "- More complex branching strategies\n",
        "- Dynamic generation of questions from different sources (e.g. csv)\n",
        "- Etc\n",
        "\n",
        "Let's use YAML to define the pipeline:"
      ],
      "metadata": {
        "id": "EQ95xqAwf-A4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YAML_PIPELINE = \"\"\"\\\n",
        "name: dummy-pipeline\n",
        "initial_step: reason_no_pay\n",
        "steps:\n",
        "  - name: reason_no_pay\n",
        "    llm: gpt4\n",
        "    question: Did the customer provided any reason to not paying the loan dues?\n",
        "    next_steps:\n",
        "      - if_llm_answer:\n",
        "          has_value: \"yes\"\n",
        "          go_to: [inability_to_pay, discipline_to_pay, unwilling_to_pay]\n",
        "      - if_llm_answer:\n",
        "          has_value: \"no\"\n",
        "          go_to: []\n",
        "\n",
        "  - name: inability_to_pay\n",
        "    llm: gpt35\n",
        "    question: Did the customer demonstrated inability to pay outstanding loan dues?\n",
        "\n",
        "  - name: discipline_to_pay\n",
        "    llm: gpt35\n",
        "    question: Did the customer demonstrated discipline to pay outstanding loan dues?\n",
        "\n",
        "  - name: unwilling_to_pay\n",
        "    llm: gpt35\n",
        "    question: Did the customer demonstrated unwillingness to pay outstanding loan dues.?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4L9bsL2Tf6zU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective is to create a pipeline that adapts its path based on the responses from the LLM.\n",
        "\n",
        "Let's use a structured approach to construct decision-driven workflows using `Pydantic`:\n",
        "\n",
        "- `PipelineStepConfig` & `PipelineConfig`: Specify the structure for each step in the pipeline and the overall pipeline configuration, respectively.\n",
        "- `NextStepsIfLLMAnswerHasValue`: Encapsulates the conditional logic for moving between steps, leveraging the LLM's responses.\n",
        "- `IfAnswerHasValueGoTo`: Defines conditions for navigating to the next steps in the pipeline, based on the LLM's answers.\n"
      ],
      "metadata": {
        "id": "wBz2NdE6hMaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional, Union\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "# Defines the structure for conditional navigation based on LLM's answer\n",
        "class IfAnswerHasValueGoTo(BaseModel):\n",
        "    has_value: Optional[str] = Field(default=None, description=\"Expected answer value to determine next step\")\n",
        "    go_to: List[str] = Field(default=[], description=\"List of next steps to go to if condition is met\")\n",
        "\n",
        "\n",
        "# Represents the conditional logic for moving between steps based on LLM's answers\n",
        "class NextStepsIfLLMAnswerHasValue(BaseModel):\n",
        "    if_llm_answer: IfAnswerHasValueGoTo\n",
        "\n",
        "\n",
        "# Configuration for each step in the pipeline\n",
        "class PipelineStepConfig(BaseModel):\n",
        "    name: str = Field(description=\"The name of the step\")\n",
        "    llm: str = Field(description=\"The LLM to use for this step\")\n",
        "    question: str = Field(description=\"Question to be processed\")\n",
        "    next_steps: Optional[Union[str, List[Union[str, NextStepsIfLLMAnswerHasValue]]]] = Field(\n",
        "        default=None, description=\"Conditional next steps based on LLM's answers\")\n",
        "\n",
        "\n",
        "# Overall pipeline configuration\n",
        "class PipelineConfig(BaseModel):\n",
        "    name: str = Field(description=\"Name of the pipeline\")\n",
        "    initial_step: str = Field(description=\"Identifier of the initial step\")\n",
        "    steps: List[PipelineStepConfig] = Field(description=\"Configuration for all steps in the pipeline\")\n"
      ],
      "metadata": {
        "id": "vZ6_v0ntcTYd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To efficiently manage our pipeline's configuration, we introduce two critical processes: **compiling** the configuration from YAML and **loading** it for use in our application.\n",
        "\n",
        "**NOTE**:\n",
        "\n",
        "**These steps ensure our pipeline is both flexible and easily maintainable. In this example the content of the JSON file is minimal, but in a real case, it can contain user and system templates, function calls, extended params, and all kind of information needed to run the pipeline.**\n",
        "\n",
        "### Compiling the Pipeline Configuration\n",
        "\n",
        "The `build_config` function takes a YAML representation of our pipeline configuration and the target filename as inputs. It performs the following actions:\n",
        "\n",
        "1. Parses the YAML string to a Python dictionary using `yaml.safe_load`.\n",
        "2. Converts the dictionary to a `PipelineConfig` object, ensuring all data is valid according to our predefined model.\n",
        "3. Saves this configuration object as a JSON file, making it easily accessible and modifiable for future use."
      ],
      "metadata": {
        "id": "s3xvgEEGhG9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "def build_config(yaml_pipeline: str, filename: str) -> PipelineConfig:\n",
        "    \"\"\"Parses YAML pipeline configuration and saves it as a JSON file.\"\"\"\n",
        "    pipeline_config = PipelineConfig(**yaml.safe_load(yaml_pipeline))\n",
        "    with open(filename, \"w\") as fp:\n",
        "        json.dump(pipeline_config.model_dump(), fp, indent=4)\n",
        "    print(f\"{filename} file generated successfully!\")\n",
        "    return pipeline_config"
      ],
      "metadata": {
        "id": "VbZjg9bbhF7Z"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Pipeline Configuration\n",
        "\n",
        "The `load_config` function reverses the process, reading the JSON file and converting it back into a `PipelineConfig` object. This enables us to work with a strongly typed configuration in our Python code, providing better error checking and editor support."
      ],
      "metadata": {
        "id": "ggE3lN8Qi5Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_config(filename: str = \"pipeline.json\") -> PipelineConfig:\n",
        "    \"\"\"Reads a JSON file and converts it into a PipelineConfig object.\"\"\"\n",
        "    with open(filename, \"r\") as fp:\n",
        "        json_data = json.load(fp)\n",
        "    return PipelineConfig.parse_obj(json_data)"
      ],
      "metadata": {
        "id": "VTovD0jTi-1o"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To demonstrate these processes, we first compile our YAML pipeline configuration into a JSON file and then load this file to obtain a `PipelineConfig` object ready for use:"
      ],
      "metadata": {
        "id": "U8AhmpuYi_yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the pipeline configuration as a JSON file\n",
        "build_config(yaml_pipeline=YAML_PIPELINE, filename=\"pipeline.json\")\n",
        "\n",
        "# Load the configuration for use\n",
        "pipeline = load_config(filename=\"pipeline.json\")"
      ],
      "metadata": {
        "id": "K4sqivqqjSKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ccedf5f-cdaf-4bad-9aae-e5c31a36e390"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pipeline.json file generated successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the Pipeline"
      ],
      "metadata": {
        "id": "U01ywxOnjU1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step Execution\n",
        "\n",
        "The `execute_step` function is designed to represent a crucial phase in our pipeline's operation, acting as a stand-in for interactions with LLMs. Within this context, it would typically dispatch queries to the LLM and process the responses received. However, for illustrative purposes, the current implementation of this function runs a very basic logic.\n",
        "\n",
        "Actual implementation would involve intricate interactions with LLMs, encompassing data processing, application of business logic, and more. Ideally, this function would be dynamically instantiated through a factory pattern, enabling the seamless integration of diverse modules based on specific pipeline configurations."
      ],
      "metadata": {
        "id": "ySSgybCIkKnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_step(step_name: str, question: str, llm: str, callbacks=[]) -> BinaryAnswer:\n",
        "    q = Question(\n",
        "        name=step_name,\n",
        "        description=\"Answer and explanation to the question relating to the customer's non-payment of loan dues\",\n",
        "        answer_description=f\"Yes/No answer to the question: {question}\",\n",
        "        details_description=\"Short explanation for the answer provided\",\n",
        "    )\n",
        "\n",
        "    model = {\n",
        "        \"gpt4\": GPT4_0125,\n",
        "        \"gpt35\": GPT35TURBO0125,\n",
        "    }[llm]\n",
        "\n",
        "    _llm = ChatOpenAI(\n",
        "      model=model,\n",
        "      openai_api_key=OPENAI_API_KEY,\n",
        "      openai_organization=OPENAI_API_ORG,\n",
        "      temperature=0,\n",
        "      model_kwargs={\"seed\": 42},\n",
        "      streaming=True,\n",
        "    )\n",
        "\n",
        "    bind = _llm.bind(\n",
        "        function_call={\"name\": q.name},\n",
        "        functions=[generate_function(q)],\n",
        "    )\n",
        "\n",
        "    chain = prompt_with_question | bind | JsonOutputFunctionsParser()\n",
        "\n",
        "    result = chain.invoke(\n",
        "        {\"transcript\": CALL_TRANSCRIPT, \"question\": question},\n",
        "        {\"callbacks\": callbacks}\n",
        "    )\n",
        "\n",
        "    return BinaryAnswer(name=step_name, **result)"
      ],
      "metadata": {
        "id": "JelJjEntkYUE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determining Next Steps\n",
        "\n",
        "Based on the response from executing a step, `get_next_steps` decides which steps should follow. It examines the current step's configuration for next steps, checking conditions against the LLM's response to select the appropriate path forward."
      ],
      "metadata": {
        "id": "w6CbXuG0kZQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_next_steps(step: str, response: BinaryAnswer):\n",
        "    next_steps_result = []\n",
        "    next_steps = step.next_steps\n",
        "\n",
        "    if isinstance(next_steps, str):\n",
        "        next_steps_result.append(next_steps)\n",
        "    elif isinstance(next_steps, list):\n",
        "        for next_step_condition in next_steps:\n",
        "            if next_step_condition.if_llm_answer.has_value == response.answer:\n",
        "                next_steps_result.extend(next_step_condition.if_llm_answer.go_to)\n",
        "    return next_steps_result"
      ],
      "metadata": {
        "id": "vWc-A6BgkgGx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running a Pipeline Step\n",
        "\n",
        "The `run_pipeline_step` function integrates executing a step and determining its subsequent steps into a cohesive operation, returning the response and the next steps to be executed."
      ],
      "metadata": {
        "id": "uYZ7MvAWkj9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline_step(step: PipelineStepConfig, callbacks=[]):\n",
        "    response = execute_step(step.name, step.question, step.llm, callbacks)\n",
        "    next_steps = get_next_steps(step, response)\n",
        "    return response, next_steps"
      ],
      "metadata": {
        "id": "0Pb0QFBCkpE-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Orchestrating the Pipeline\n",
        "\n",
        "Finally, `run_pipeline` orchestrates the entire pipeline execution process, managing the sequence of steps to be executed based on the initial configuration and adapting the flow dynamically in response to each step's outcome."
      ],
      "metadata": {
        "id": "KXvsx2-nkqM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline(transcript: str, pipeline: PipelineConfig, callbacks=[]):\n",
        "    steps_to_execute: List[str] = [pipeline.initial_step]\n",
        "    responses = []\n",
        "\n",
        "    while steps_to_execute:\n",
        "        current_step_name = steps_to_execute.pop(0)\n",
        "        # Find the step configuration by name\n",
        "        current_step_config = next(filter(lambda s: s.name == current_step_name, pipeline.steps), None)\n",
        "        if current_step_config is None:\n",
        "            print(f\"Step '{current_step_name}' not found in pipeline configuration.\")\n",
        "            continue\n",
        "\n",
        "        response, next_steps = run_pipeline_step(current_step_config, callbacks)\n",
        "        responses.append(response)\n",
        "        steps_to_execute.extend(next_steps)\n",
        "\n",
        "    return responses"
      ],
      "metadata": {
        "id": "wwiP9t2qu1D1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Execution of the Pipeline\n",
        "\n",
        "After defining our pipeline's structure and the logic to execute it, we proceed to the final step: running the pipeline with an actual transcript. This process involves loading our previously compiled pipeline configuration from a JSON file and then executing the pipeline with a given transcript. The outcome is a list of responses based on the decisions made at each step in the pipeline.\n",
        "\n",
        "Here's how you can perform this final execution:"
      ],
      "metadata": {
        "id": "rK-Hz-SFk9lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the compiled pipeline configuration\n",
        "pipeline_config = load_config(filename=\"pipeline.json\")\n",
        "\n",
        "# Assuming 'CALL_TRANSCRIPT' contains the transcript to be processed\n",
        "# Execute the pipeline with the loaded configuration and the transcript\n",
        "responses = run_pipeline(transcript=CALL_TRANSCRIPT, pipeline=pipeline_config)\n",
        "\n",
        "# Iterate through each result in the results list, printing with added clarity\n",
        "for result in responses:\n",
        "    print(f\"Question: {result.name}\")\n",
        "    print(f\"Answer: {result.answer}\")\n",
        "    # Optionally, check if 'details' has meaningful content before printing\n",
        "    if result.details.strip():\n",
        "        print(f\"Details: {result.details}\")\n",
        "    print(\"-\" * 40)  # Add a separator for readability"
      ],
      "metadata": {
        "id": "aeaBov1mwCDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfa78c08-f62c-4f84-d5a7-9395ba863dca"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: reason_no_pay\n",
            "Answer: yes\n",
            "Details: The customer mentioned being out of town as the reason for not making the loan payment.\n",
            "----------------------------------------\n",
            "Question: inability_to_pay\n",
            "Answer: no\n",
            "Details: The customer did not demonstrate inability to pay outstanding loan dues. Instead, the customer mentioned that their son will return in two days to make the payment and also considered using Phone Pay or Google Pay for an immediate payment.\n",
            "----------------------------------------\n",
            "Question: discipline_to_pay\n",
            "Answer: no\n",
            "Details: The customer did not demonstrate immediate action or commitment to pay the outstanding loan dues during the call. Although the customer mentioned that their son will make the payment in two days, there was no clear promise or commitment to pay immediately or set up the payment during the call.\n",
            "----------------------------------------\n",
            "Question: unwilling_to_pay\n",
            "Answer: no\n",
            "Details: The customer did not demonstrate unwillingness to pay outstanding loan dues. The customer acknowledged the outstanding payment, mentioned that their son will make the payment upon returning, and expressed intent to discuss payment options with their son.\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions and Future Directions\n",
        "\n",
        "The development of this dynamic pipeline represents a significant step towards creating adaptable and scalable data processing workflows. By leveraging the capabilities of LLMs, we've established a flexible foundation that can dynamically adjust its processing path based on real-time data analysis and decision-making.\n",
        "\n",
        "## Enhancements and Extensions\n",
        "\n",
        "To further increase the utility and efficiency of our pipeline, we propose several avenues for future development:\n",
        "\n",
        "- **Expand Pipeline Configurations**: Introduce additional branching logic, data loading mechanisms, and prompt templates to cover a wider range of use cases and increase the pipeline's flexibility.\n",
        "- **Incorporate Data Processing Modules**: By adding specialized data extraction and processing modules, each step of the pipeline can be enhanced to perform more complex operations, enriching the data as it flows through the pipeline.\n",
        "- **Asynchronous LLM Calls**: Transitioning to asynchronous calls to LLMs can significantly improve the pipeline's performance, especially when scaling up to handle larger volumes of data.\n",
        "- **Package Modularization**: Packaging the entire pipeline framework as a Python package will facilitate reuse, distribution, and deployment, making it easier to run at scale.\n",
        "- **Evaluation Workflows**: Implementing a comprehensive evaluation workflow within the pipeline will allow for continuous monitoring and improvement of the decision-making logic based on performance metrics.\n",
        "\n",
        "## Integration Example: Airflow DAG\n",
        "\n",
        "To demonstrate the practical application and integration of our pipeline into larger data processing ecosystems, consider the following example of using it within an Airflow Directed Acyclic Graph (DAG):\n",
        "\n",
        "```python\n",
        "import asyncio\n",
        "from airflow.decorators import task\n",
        "from deepgenai import load_config, run_pipeline\n",
        "\n",
        "with DAG(...) as dag:\n",
        "  ...\n",
        "  @task(task_id=\"transform_files\")\n",
        "  def transform_files(pipeline_config: str, batches: List[str]):\n",
        "    ...\n",
        "    loop = asyncio.get_event_loop()\n",
        "    tasks = [\n",
        "      run_pipeline(transcript, pipeline_config)\n",
        "      for transcript in batches\n",
        "    ]\n",
        "    loop.run_until_complete(asyncio.gather(*tasks))\n",
        "\n",
        "start = EmptyOperator(task_id=\"start\")\n",
        "end = EmptyOperator(task_id=\"end\")\n",
        "\n",
        "batches = get_file_batches()\n",
        "pipeline_config = get_pipeline_config()\n",
        "transform_task = transform_files.partial(pipeline_config=pipeline_config).expand(transcript_batch=batches)\n",
        "\n",
        "start >> [batches, pipeline_config] >> transform_task >> end\n",
        "```"
      ],
      "metadata": {
        "id": "rHQSbntLEMot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Bonus - Real-Time feedback\n",
        "\n",
        "In some cases we might want to get results during the pipeline execution and not waiting until the end of the run process. This can be useful in some cases where feedback needs to be provided as soon as is available, for example for real-time processing tasks.\n",
        "\n",
        "The following block of code shows how to implement the functionality using [LangChain Callbacks](https://python.langchain.com/docs/modules/callbacks/) and calling the same `run_pipeline` function than previously, just adding the callbacks list."
      ],
      "metadata": {
        "id": "Kz_mvZOti8il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TYPE_CHECKING, Any, Dict, Optional\n",
        "\n",
        "from langchain_core.outputs import LLMResult\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "\n",
        "class MyCustomHandler(BaseCallbackHandler):\n",
        "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:\n",
        "        if isinstance(outputs, dict) and \"answer\" in outputs.keys():\n",
        "            print(\"\\n\\033[1m> Finished chain.\\033[0m\", str(outputs))\n",
        "\n",
        "callbacks = [\n",
        "    MyCustomHandler(),\n",
        "]\n",
        "\n",
        "responses = run_pipeline(transcript=CALL_TRANSCRIPT, pipeline=pipeline_config, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "HB7qu_IqcEhh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcba3af5-6f1e-4bcf-a7cb-7cbbf6c7e3ab"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m {'answer': 'yes', 'details': 'The customer mentioned being out of town as the reason for not making the loan payment.'}\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m {'answer': 'yes', 'details': 'The customer mentioned being out of town as the reason for not making the loan payment.'}\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m {'answer': 'no', 'details': 'The customer did not demonstrate inability to pay outstanding loan dues. Instead, the customer mentioned that their son will return in two days to make the payment and expressed intentions to handle the payment later.'}\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m {'answer': 'no', 'details': 'The customer did not demonstrate inability to pay outstanding loan dues. Instead, the customer mentioned that their son will return in two days to make the payment and expressed intentions to handle the payment later.'}\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m {'answer': 'no', 'details': 'The customer did not demonstrate discipline to pay outstanding loan dues. Although the customer acknowledged the outstanding payment and mentioned that their son will make the payment in two days, there was no clear commitment or immediate action taken to resolve the overdue payment during the call.'}\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m {'answer': 'no', 'details': 'The customer did not demonstrate discipline to pay outstanding loan dues. Although the customer acknowledged the outstanding payment and mentioned that their son will make the payment in two days, there was no clear commitment or immediate action taken to resolve the overdue payment during the call.'}\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m {'answer': 'no', 'details': 'The customer did not demonstrate unwillingness to pay outstanding loan dues. The customer acknowledged the outstanding payment, mentioned that their son will make the payment in two days, and expressed intent to discuss payment options with their son.'}\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m {'answer': 'no', 'details': 'The customer did not demonstrate unwillingness to pay outstanding loan dues. The customer acknowledged the outstanding payment, mentioned that their son will make the payment in two days, and expressed intent to discuss payment options with their son.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nthOvAdVd3DC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}